{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dae6744",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning using LSTM with GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ff7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Geethu Thottungal Harilal\n",
    "# data from : https://power.larc.nasa.gov/data-access-viewer/\n",
    "\n",
    "# This code will use daily data from all locations after feature engineering and \n",
    "# implement GridSearchCV hyperparameter tuning with  LSTM to find best parameters for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370fb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout,LSTM, Dense \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3378aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "data_en=pd.read_csv(\"England_dataset_final_1981-2023.csv\",parse_dates=[\"date\"]) #England\n",
    "data_wl=pd.read_csv(\"Wales_dataset_final_1981-2023.csv\",parse_dates=[\"date\"]) #wales\n",
    "data_sc=pd.read_csv(\"Scotland_dataset_final_1981-2023.csv\",parse_dates=[\"date\"]) #scotland\n",
    "data_ir=pd.read_csv(\"Ireland_dataset_final_1981-2023.csv\",parse_dates=[\"date\"]) #ireland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f4e78",
   "metadata": {},
   "source": [
    "## England\n",
    "This code section will implement hyperparameter tuning in England dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99463a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Rainfall'\n",
    "\n",
    "# Extract the target variable\n",
    "target_en = data_en[target_col].values\n",
    "df_dates = data_en['date']\n",
    "df_en =data_en.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d67c19",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32654a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train\n",
    "train_split=round(len(df_en)*0.67)\n",
    "df_for_training_en=df_en[:train_split]\n",
    "df_temp_en=df_en[train_split:]\n",
    "test_split=round(len(df_temp_en)*0.5)\n",
    "df_for_validation_en=df_temp_en[:test_split]\n",
    "df_for_testing_en=df_temp_en[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d625dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_for_training Shape--  (10419, 7)\n",
      "df_for_validation Shape--  (2566, 7)\n",
      "df_for_testing Shape--  (2566, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_for_training Shape-- \",df_for_training_en.shape)\n",
    "print(\"df_for_validation Shape-- \",df_for_validation_en.shape)\n",
    "print(\"df_for_testing Shape-- \",df_for_testing_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19186d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test dates\n",
    "temp_date=df_dates[train_split:]\n",
    "df_for_testdate=temp_date[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4a042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the dataset with minmaxscaler within the range 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled_en = scaler.fit_transform(df_for_training_en)\n",
    "df_for_validation_scaled_en = scaler.fit_transform(df_for_validation_en)\n",
    "df_for_testing_scaled_en=scaler.transform(df_for_testing_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17882a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of time steps and features\n",
    "n_steps = 15  # Number of time steps to consider\n",
    "n_features = df_for_testing_scaled_en.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8090c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in X and Y\n",
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            #print(i)\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX_en,trainY_en=createXY(df_for_training_scaled_en,n_steps)\n",
    "valX_en,valY_en=createXY(df_for_validation_scaled_en,n_steps)\n",
    "testX_en,testY_en=createXY(df_for_testing_scaled_en,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0446047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "def create_lstm_model(units=32,learning_rate=.01, optimizer='adam', activation='relu'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(units=units,activation=activation,input_shape=(n_steps,n_features)))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1,activation='tanh'))\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "    grid_model.compile(loss = 'mean_squared_error',optimizer = opt)\n",
    "    return grid_model\n",
    "\n",
    "# Create KerasRegressor wrapper for scikit-learn\n",
    "grid_model = KerasRegressor(build_fn=create_lstm_model, verbose=1,validation_data=(valX_en, valY_en))\n",
    "\n",
    "# Define hyperparameters and values to search\n",
    "parameters = {'units': [32, 64, 128],\n",
    "              'optimizer': ['adam','SGD'],\n",
    "              'learning_rate':[.1,.01,.001],\n",
    "              'activation':['relu', 'tanh', 'sigmoid','swish'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf009be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 13ms/step - loss: 0.0260 - val_loss: 0.0268\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv=3, refit=True, n_jobs = -1)\n",
    "# fit the model in out trainX and trainY data\n",
    "grid_result = grid_search.fit(trainX_en, trainY_en ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b441c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for England data:   {'activation': 'tanh', 'learning_rate': 0.1, 'optimizer': 'SGD', 'units': 128}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding RMSE\n",
    "print(\"Best params for England data:  \",grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc9e04",
   "metadata": {},
   "source": [
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e3c18",
   "metadata": {},
   "source": [
    "## Wales\n",
    "This code section will implement hyperparameter tuning in Wales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e0c6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Rainfall'\n",
    "\n",
    "# Extract the target variable\n",
    "target_wl = data_wl[target_col].values\n",
    "df_dates = data_wl['date']\n",
    "df_wl =data_wl.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82824450",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ed3b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train\n",
    "train_split=round(len(df_wl)*0.67)\n",
    "df_for_training_wl=df_wl[:train_split]\n",
    "df_temp_wl=df_wl[train_split:]\n",
    "test_split=round(len(df_temp_wl)*0.5)\n",
    "df_for_validation_wl=df_temp_wl[:test_split]\n",
    "df_for_testing_wl=df_temp_wl[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40b00b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_for_training Shape--  (10419, 7)\n",
      "df_for_validation Shape--  (2566, 7)\n",
      "df_for_testing Shape--  (2566, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_for_training Shape-- \",df_for_training_wl.shape)\n",
    "print(\"df_for_validation Shape-- \",df_for_validation_wl.shape)\n",
    "print(\"df_for_testing Shape-- \",df_for_testing_wl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "defdbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the dataset with minmaxscaler within the range 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled_wl = scaler.fit_transform(df_for_training_wl)\n",
    "df_for_validation_scaled_wl = scaler.fit_transform(df_for_validation_wl)\n",
    "df_for_testing_scaled_wl=scaler.transform(df_for_testing_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1967829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of time steps and features\n",
    "n_steps = 15  # Number of time steps to consider\n",
    "n_features = df_for_testing_scaled_wl.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ce3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in X and Y\n",
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            #print(i)\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX_wl,trainY_wl=createXY(df_for_training_scaled_wl,n_steps)\n",
    "valX_wl,valY_wl=createXY(df_for_validation_scaled_wl,n_steps)\n",
    "testX_wl,testY_wl=createXY(df_for_testing_scaled_wl,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce4429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "def create_lstm_model(units=32,learning_rate=.01, optimizer='adam', activation='relu'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(units=units,activation=activation,input_shape=(n_steps,n_features)))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1,activation='tanh'))\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "    grid_model.compile(loss = 'mean_squared_error',optimizer = opt)\n",
    "    return grid_model\n",
    "\n",
    "# Create KerasRegressor wrapper for scikit-learn\n",
    "grid_model = KerasRegressor(build_fn=create_lstm_model, verbose=1,validation_data=(valX_wl, valY_wl))\n",
    "\n",
    "# Define hyperparameters and values to search\n",
    "parameters = {'units': [32, 64, 128],\n",
    "              'optimizer': ['adam','SGD'],\n",
    "              'learning_rate':[.1,.01,.001],\n",
    "              'activation':['relu', 'tanh', 'sigmoid','swish'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3a1f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 10ms/step - loss: 0.0282 - val_loss: 0.0247\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv=3, refit=True, n_jobs = -1)\n",
    "# fit the model in out trainX and trainY data\n",
    "grid_result = grid_search.fit(trainX_wl, trainY_wl ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ef7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Wales data:   {'activation': 'tanh', 'learning_rate': 0.001, 'optimizer': 'SGD', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding RMSE\n",
    "print(\"Best params for Wales data:  \",grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43c26d",
   "metadata": {},
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d5548",
   "metadata": {},
   "source": [
    "## Scotland\n",
    "This code section will implement hyperparameter tuning in Scotland dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10e06ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Rainfall'\n",
    "\n",
    "# Extract the target variable\n",
    "target_sc = data_sc[target_col].values\n",
    "df_dates = data_sc['date']\n",
    "df_sc =data_sc.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73bf31",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6668e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train\n",
    "train_split=round(len(df_sc)*0.67)\n",
    "df_for_training_sc=df_sc[:train_split]\n",
    "df_temp_sc=df_sc[train_split:]\n",
    "test_split=round(len(df_temp_sc)*0.5)\n",
    "df_for_validation_sc=df_temp_sc[:test_split]\n",
    "df_for_testing_sc=df_temp_sc[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7101e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_for_training Shape--  (10419, 7)\n",
      "df_for_validation Shape--  (2566, 7)\n",
      "df_for_testing Shape--  (2566, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_for_training Shape-- \",df_for_training_sc.shape)\n",
    "print(\"df_for_validation Shape-- \",df_for_validation_sc.shape)\n",
    "print(\"df_for_testing Shape-- \",df_for_testing_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b0820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test dates\n",
    "temp_date=df_dates[train_split:]\n",
    "df_for_testdate=temp_date[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "965ec9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the dataset with minmaxscaler within the range 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled_sc = scaler.fit_transform(df_for_training_sc)\n",
    "df_for_validation_scaled_sc = scaler.fit_transform(df_for_validation_sc)\n",
    "df_for_testing_scaled_sc=scaler.transform(df_for_testing_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30ac709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of time steps and features\n",
    "n_steps = 15  # Number of time steps to consider\n",
    "n_features = df_for_testing_scaled_sc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ad6dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in X and Y\n",
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            #print(i)\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX_sc,trainY_sc=createXY(df_for_training_scaled_sc,n_steps)\n",
    "valX_sc,valY_sc=createXY(df_for_validation_scaled_sc,n_steps)\n",
    "testX_sc,testY_sc=createXY(df_for_testing_scaled_sc,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48922fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "def create_lstm_model(units=32,learning_rate=.01, optimizer='adam', activation='relu'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(units=units,activation=activation,input_shape=(n_steps,n_features)))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1,activation='tanh'))\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "    grid_model.compile(loss = 'mean_squared_error',optimizer = opt)\n",
    "    return grid_model\n",
    "\n",
    "# Create KerasRegressor wrapper for scikit-learn\n",
    "grid_model = KerasRegressor(build_fn=create_lstm_model, verbose=1,validation_data=(valX_sc, valY_sc))\n",
    "\n",
    "# Define hyperparameters and values to search\n",
    "parameters = {'units': [32, 64, 128],\n",
    "              'optimizer': ['adam','SGD'],\n",
    "              'learning_rate':[.1,.01,.001],\n",
    "              'activation':['relu', 'tanh', 'sigmoid','swish'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38107277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 8ms/step - loss: 0.0241 - val_loss: 0.0214\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv=3, refit=True, n_jobs = -1)\n",
    "# fit the model in out trainX and trainY data\n",
    "grid_result = grid_search.fit(trainX_sc, trainY_sc ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d467e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Scotland data:   {'activation': 'tanh', 'learning_rate': 0.1, 'optimizer': 'SGD', 'units': 32}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding RMSE\n",
    "print(\"Best params for Scotland data:  \",grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bc535",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f226c8",
   "metadata": {},
   "source": [
    "## Northern Ireland\n",
    "This code section will implement hyperparameter tuning in Northern Ireland dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9c58cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Rainfall'\n",
    "\n",
    "# Extract the target variable\n",
    "target_ir = data_ir[target_col].values\n",
    "df_dates = data_ir['date']\n",
    "df_ir =data_ir.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a730a",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06d19deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train\n",
    "train_split=round(len(df_ir)*0.67)\n",
    "df_for_training_ir=df_ir[:train_split]\n",
    "df_temp_ir=df_ir[train_split:]\n",
    "test_split=round(len(df_temp_ir)*0.5)\n",
    "df_for_validation_ir=df_temp_ir[:test_split]\n",
    "df_for_testing_ir=df_temp_ir[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dc47bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_for_training Shape--  (10419, 7)\n",
      "df_for_validation Shape--  (2566, 7)\n",
      "df_for_testing Shape--  (2566, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_for_training Shape-- \",df_for_training_ir.shape)\n",
    "print(\"df_for_validation Shape-- \",df_for_validation_ir.shape)\n",
    "print(\"df_for_testing Shape-- \",df_for_testing_ir.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74dc22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the dataset with minmaxscaler within the range 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled_ir = scaler.fit_transform(df_for_training_ir)\n",
    "df_for_validation_scaled_ir = scaler.fit_transform(df_for_validation_ir)\n",
    "df_for_testing_scaled_ir=scaler.transform(df_for_testing_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10ac034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of time steps and features\n",
    "n_steps = 15  # Number of time steps to consider\n",
    "n_features = df_for_testing_scaled_ir.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c70f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in X and Y\n",
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            #print(i)\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX_ir,trainY_ir=createXY(df_for_training_scaled_ir,n_steps)\n",
    "valX_ir,valY_ir=createXY(df_for_validation_scaled_ir,n_steps)\n",
    "testX_ir,testY_ir=createXY(df_for_testing_scaled_ir,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b76e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "def create_lstm_model(units=32,learning_rate=.01, optimizer='adam', activation='relu'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(units=units,activation=activation,input_shape=(n_steps,n_features)))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1,activation='tanh'))\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "    grid_model.compile(loss = 'mean_squared_error',optimizer = opt)\n",
    "    return grid_model\n",
    "\n",
    "# Create KerasRegressor wrapper for scikit-learn\n",
    "grid_model = KerasRegressor(build_fn=create_lstm_model, verbose=1,validation_data=(valX_ir, valY_ir))\n",
    "\n",
    "# Define hyperparameters and values to search\n",
    "parameters = {'units': [32, 64, 128],\n",
    "              'optimizer': ['adam','SGD'],\n",
    "              'learning_rate':[.1,.01,.001],\n",
    "              'activation':['relu', 'tanh', 'sigmoid','swish'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baaebb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 11ms/step - loss: 0.0275 - val_loss: 0.0188\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv=3, refit=True, n_jobs = -1)\n",
    "# fit the model in out trainX and trainY data\n",
    "grid_result = grid_search.fit(trainX_ir, trainY_ir ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ebaf45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Ireland data:   {'activation': 'tanh', 'learning_rate': 0.01, 'optimizer': 'SGD', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding RMSE\n",
    "print(\"Best params for Ireland data:  \",grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9f275",
   "metadata": {},
   "source": [
    "#######################################################################################################"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
